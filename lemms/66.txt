знакомство с хранилище ceph в картинка alexbin облачный файловый хранилище продолжать набирать популярность и требование к он продолжать растить современный система уже не в состояние полностью удовлетворить весь этот требование без значительный затрата ресурс на поддержка и масштабирование этот система под система я подразумевать кластер с тем или иной уровень доступ к данные для пользователь важный надёжность хранение и высокий доступность чтобы файл можно быть всегда легко и быстро получить а риск потеря дать стремиться к нуль в свой очередь для поставщик и администратор такой хранилище важный простота поддержка масштабируемость и низкий стоимость аппаратный и программный компонент ceph это программно определять распределенный файловый система с открытый исходный код лишить узкий место и единый точка отказ который представлять из себя легко масштабировать до петабайтный размер кластер узел выполнять различный функция обеспечивать хранение и репликация дать а также распределение нагрузка что гарантировать высокий доступность и надёжность система бесплатный хотя разработчик мочь предоставить платный поддержка никакой специальный оборудование не требоваться при выход любой диск узел или группа узел из строй ceph не только обеспечить сохранность дать но и сам восстановить утратить копия на другой узел до тот пора пока выйти из строй узел или диск не заменить на рабочий при это ребилд происходить без секунда простой и прозрачный для клиент поскольку система программно определять и работать поверх стандартный файловый система и сетевой уровень можно взять пачка разный сервер набить они разный диск разный размер соединить вс это счастие какой нибыть сеть хороший быстрый и поднять кластер можно воткнуть в этот сервер по два сетевой карта и соединить они два сеть для ускорение межсерверный обмен дать а эксперимент с настройка и схема можно легко проводить даже в виртуальный среда мой опыт эксперимент показывать что самый долгий в это процесс это установка ос если у мы есть три сервер с диск и настроить сеть то поднятие работоспособный кластер с дефолтный настройка занять 5 10 минута если весь делать правильно поверх операционный система работать демон ceph выполнять различный роль кластер такой образ один сервер мочь выступать например и в роль монитор mon и в роль хранилище дать osd а другой сервер тем время мочь выступать в роль хранилище дать и в роль сервер метаданные mds в больший кластер демон запускаться на отдельный машина но в малое кластер где количество сервер сильно ограничить некоторый сервер мочь выполнять сразу два или три роль зависеть от мощность сервер и сам роль разумеется весь быть работать шустрый на отдельный сервер но не всегда это возможно реализовать кластер можно собрать даже из один машина и всего один диск и он быть работать другой разговор что это не быть иметь смысл следовать отметить и то что благодаря программный определяемость хранилище можно поднять даже поверх raid или iscsi устройство однако в большинство случай это тоже не быть иметь смысл в документация перечислить 3 вид демон первоначальный кластер можно создать из несколько машина совмещать на они роль кластер затем с рост кластер и добавление новый сервер какой то роль можно дублировать на другой машина или полностью выносить на отдельный сервер для начало коротко и непонятно кластер мочь иметь один или много пул дать разный назначение и с разный настройка пул делиться на плейсмент группа в плейсмент группа храниться объект к который обращаться клиент на это логический уровень заканчиваться и начинаться физический потому как за каждый плейсмент группа закрепить один главный диск и несколько диск реплика сколько именно зависеть от фактор репликация пул другой слово на логический уровень объект храниться в конкретный плейсмент группа а на физический на диск который за она закрепить при это диск физически мочь находиться на разный узел или даже в разный датацентр далее подробно понятно фактор репликация это уровень избыточность дать количество копия дать который быть храниться на разный диск за этот параметр отвечать переменный size фактор репликация мочь быть разный для каждый пул и он можно менять на лёт вообще в ceph практически весь параметр можно менять на лёт мгновенно получать реакция кластер сначала у мы мочь быть size 2 и в это случай пул быть хранить по два копия один кусок дать на разный диск этот параметр пул можно поменять на size 3 и в этот же момент кластер начать перераспределять дать раскладывать ещё один копия уже иметься дать по диск не останавливать работа клиент пул это логический абстрактный контейнер для организация хранение дать пользователь любой дать храниться в пул в вид объект несколько пул мочь быть размазать по один и тем же диск а мочь и по разный как настроить с помощь разный набор плейсмент группа каждый пул иметь ряд настраивать параметр фактор репликация количество плейсмент группа минимальный количество живой реплика объект необходимый для работа и том далее каждый пул можно настроить свой политика репликация по город датацентр стойка или даже диск например пул под хостинг мочь иметь фактор репликация size 3 а зона отказ быть датацентр и тогда ceph быть гарантировать что каждый кусочек дать иметь по один копия в три датацентр тем время пул для виртуальный машина мочь иметь фактор репликация size 2 а уровень отказ уже быть серверный стойка и в это случай кластер быть хранить только два копия при это если у мы два стойка с хранилище виртуальный образ в один датацентр и два стойка в друг система не быть обращать внимание на датацентр и оба копия дать мочь улететь в один датацентр однако гарантированно в разный стойка как мы и хотеть плейсмент группа это такой связующий звено между физический уровень хранение диск и логический организация дать пул каждый объект на логический уровень храниться в конкретный плейсмент группа на физический же уровень он лежать в нужный количество копия на разный физический диск который в этот плейсмент группа включить на самый дело не диск а osd но обычно один osd это и есть один диск и для простота я быть называть это диск хотя напомнить за он мочь быть и raid массив или iscsi устройство при фактор репликация size 3 каждый плейсмент группа включать в себя три диск но при это каждый диск находиться в множество плейсмент группа и для какой то группа он быть первичный для другой реплика если osd входить например в состав три плейсмент группа то при падение такой osd плейсмент группа исключить он из работа и на он место каждый плейсмент группа выбрать рабочий osd и размазать по он дать с помощь данный механизм и достигаться достаточно равномерный распределение дать и нагрузка это весьма простой и одновременно гибкий решение монитор это демон выполнять роль координатор с который начинаться кластер как только у мы появляться хотя бы один рабочий монитор у мы появляться ceph кластер монитор хранить информация о здоровье и состояние кластер обмениваться различный карта с другой монитор клиент обращаться к монитор чтобы узнать на какой osd писать читать дать при разворачивание новое хранилище один дело создаваться монитор или несколько кластер мочь прожить на один монитор но рекомендоваться делать 3 или 5 монитор в избежание падение весь система по причина падение единственный монитор главный чтобы количество оный быть нечётный даба избежать ситуация раздвоение сознание split brain монитор работать в кворум поэтому если упасть большой половина монитор кластер заблокироваться для предотвращение рассогласованность дать osd это юнит хранилище который хранить сам дать и обрабатывать запрос клиент обмениваться дать с другой osd обычно это диск и обычно за каждый osd отвечать отдельный osd демон который мочь запускаться на люба машина на который установленный этот диск это два что нужно добавлять в кластер при разворачивание один монитор и один osd минимальный набор для тот чтобы поднять кластер и начать имя пользоваться если на сервер крутиться 12 диск под хранилище то на немой быть запустить столько же osd демон клиент работать непосредственно с сам osd миновать узкий место и достигать тем самый распределение нагрузка клиент всегда записывать объект на первичный osd для какой то плейсмент группа а уже далёкий данный osd синхронизировать дать с остальной вторичный osd из этот же плейсмент группа подтверждение успешный запись мочь отправляться клиент сразу же после запись на первичный osd а мочь после достижение минимальный количество запись параметр пул min_size например если фактор репликация size 3 а min_size 2 то подтверждение о успешный запись отправиться клиент когда объект записаться хотя бы на два osd из три включая первичный при разный вариант настройка этот параметр мы быть наблюдать и разный поведение если size 3 и min_size 2 весь быть хорошо пока 2 из 3 osd плейсмент группа живой когда остаться всего лишь 1 живой osd кластер заморозить операция дать плейсмент группа пока не ожить хотя бы ещё один osd если size min_size то плейсмент группа быть блокироваться при падение любой osd входящий в она состав а из за высокий уровень размазанность дать большинство падение хотя бы один osd быть заканчиваться заморозка всего или почти всего кластер поэтому параметр size всегда должный быть хотя бы на один пункт большой параметр min_size если size 1 кластер быть работать но смерть люба osd быть означать безвозвратный потеря дать ceph дозволять выставить этот параметр в единица но даже если администратор делать это с определённый цель на короткий время он риск брать на себя диск osd состоять из два часть журнал и сам дать соответственно дать сначала писаться в журнал затем уже в раздел дать с один сторона это давать дополнительный надёжность и некоторый оптимизация а с другой сторона дополнительный операция который сказываться на производительность вопрос производительность журнал рассмотреть ниже в основа механизм децентрализация и распределение лежать так называть crush алгоритм controlled replicated under scalable hashing играть важный роль в архитектура система этот алгоритм позволять однозначно определить местоположение объект на основа хеш имя объект и определённый карта который формироваться исходить из физический и логический структура кластер датацентр зала ряд стойка узел диск карта не включать в себя информация о местонахождение дать путь к данные каждый клиент определять сам с помощь crush алгоритм и актуальный карта который он предварительно спрашивать у монитор при добавление диск или падение сервер карта обновляться благодаря детерминированность два разный клиент найти один и тот же однозначный путь до один объект самостоятельно избавлять система от необходимость держать весь этот путь на какой то сервер синхронизировать они между себя давать огромный избыточный нагрузка на хранилище в целое пример клиент хотеть записать некий объект object1 в пул pool1 для это он смотреть в карта плейсмент группа который он ранее любезно предоставить монитор и видеть что pool1 раздельный на 10 плейсмент группа далее с помощь crush алгоритм который на вход принимать имя объект и общий количество плейсмент группа в пул pool1 вычисляться id плейсмент группа следовать карта клиент понимать что за этот плейсмент группа закрепить три osd допустить они номер 17 9 и 22 один из который являться первичный а значит клиент быть производить запись именно на он кстати они три потому что в это пул установленный фактор репликация size 3 после успешный запись объект на osd_17 работа клиент закончить это если параметр пул min_size 1 а osd_17 реплицировать этот объект на osd_9 и osd_22 закрепить за этот плейсмент группа важный понимать что это упрощённый объяснение работа алгоритм по умолчание наш crush карта плоский весь нода находиться в один пространство однако можно этот плоскость легко превратить в дерево распределить сервер по стойка стойка по ряд ряд по зал зала по датацентр а датацентр по разный город и планет указать какой уровень считать зона отказ оперировать такой новый карта ceph быть грамотный распределять дать учитывать индивидуальный особенность организация предотвращать печальный последствие пожар в датацентр или падение метеорит на целый город более тот благодаря это гибкий механизм можно создавать дополнительный слоить как на верхний уровень датацентр и город так и на нижний например дополнительный разделение на группа диск в рамка один сервер ceph предусматривать несколько способ увеличение производительность кластер метод кеширование у каждый osd есть несколько весы и один из они отвечать за то какой osd в плейсмент группа быть первичный а как мы выяснить ранее клиент писать дать именно на первичный osd так вот можно добавить в кластер пачка ssd диск сделать они всегда первичный снизить вес primary affinity hdd диск до нуль и тогда запись быть осуществляться всегда сначала на быстрый диск а затем уже не спешить реплицироваться на медленный этот метод самый неправильный однако самый простой в реализация главный недостаток в тот что один копия дать всегда быть лежать на ssd и потребоваться очень много такой диск чтобы полностью покрыть репликация хотя этот способ кто то и применять на практика но он я скорее упомянуть для тот чтобы рассказать о возможность управление приоритет запись вообще львиный доля производительность зависеть от журнал osd осуществлять запись демон сначала писать дать в журнал а затем в сам хранилище это верно всегда кроме случай использование btrfs в качество файловый система на osd который мочь делать это параллельно благодаря техника copy on write но я так и не понять насколько она готовый к промышленный применение на каждый osd идти собственный журнал и по умолчание он находиться на тот же диск что и сам дать однако журнал с четыр х или пять диск можно вынести на один ssd неплохо ускорить операция запись метод не очень гибкий и удобный но достаточно простой недостаток метод в тот что при вылет ssd с журнал мы потерять сразу несколько osd что не очень приятно и вносить дополнительный трудность в весь дальнейший поддержка который скалироваться с рост кластер ортодоксальность данный метод в он гибкость и масштабируемость схема таков что у мы есть пул с холодное дать и пул с горячее при частый обращение к объект тот как бы нагреваться и попадать в горячий пул который состоять из быстрый ssd затем если объект остывать он попадать в холодный пул с медленный hdd дать схема позволять легко менять ssd в горячее пул который в свой очередь мочь быть любой размер ибо параметр нагрев и охлаждение регулироваться ceph предоставлять для клиент различный вариант доступ к данные блочный устройство файловый система или объектный хранилище ceph позволять в пул дать создать блочный устройство rbd и в дальнейший смонтировать он на операционный система который это поддерживать на момент написание статья быть только различный дистрибутив linux однако freebsd и vmware тоже работать в этот сторона если клиент не поддерживать rbd например windows то можно использовать промежуточный iscsi target с поддержка rbd например tgt rbd кроме тот такой блочный устройство поддерживать снапшота клиент мочь смонтировать файловый система cephfs если у он linux с версия ядро 2 6 34 или новый если версия ядро старший то можно смонтировать она через fuse filesystem in user space для тот чтобы клиент мочь подключать ceph как файловый система необходимый в кластер поднять хотя бы один сервер метаданные mds с помощь шлюз rgw rados gateway можно клиент дать возможность пользоваться хранилище через restful amazon s3 или openstack swift совместимый api весь этот уровень доступ к данные работать поверх уровень rados список можно дополнить разработать свой слой доступ к данные с помощь librados api через который и работать перечисленный выше слоить доступ на данный момент есть биндинга c python ruby java и php rados reliable autonomic distributed object store если в два слово то это слой взаимодействие клиент и кластер википедия гласить что сам ceph написать на c и python а в разработка принимать участие компания canonical cern cisco fujitsu intel red hat sandisk and suse зачем я весь это написать и нарисовать картинк затем что не смотреть на весь этот достоинство ceph либо не очень популярный либо человек кушать он втихомолку судить по количество информация о немой в интернет то что ceph гибкий простой и удобный мы выяснить кластер можно поднять на любой железо в обычный сеть потратить минимум время и сила при это ceph сам быть заботиться о сохранность дать предпринимать необходимый мера в случай сбой железо в тот что ceph гибкий простой и масштабировать сходиться множество точка зрение однако отзыв о производительность встречаться весьма разнообразный возможно кто то не справиться с журнал кто то подвести сеть и задержка в операция ввод вывод то есть заставить кластер работать легко но заставить он работать быстро возможно сложный посему я взывать к ит специалист который иметь опыт использование ceph в продакшен поделиться в комментарий о свой отрицательный впечатлениях

