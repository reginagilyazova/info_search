Запускаем полноценный кластер на Kubernetes с нуля на Ubuntu 16.04
danuk
Уже довольно много написано статей, по установке и запуску  , однако, не всё так гладко (я потратил несколько суток на запуск своего кластера). 
 
Данная статья призвана дать исчерпывающую информацию не только по установке k8s, но и объяснить каждый шаг: зачем и почему мы делаем именно так, как написано (это очень важно для успешного запуска). 
 
 
 
Кластер подразумевает, что у Вас более одного физического сервера, между которыми и будут распределятся ресурсы. Серверы называются нодами (nodes). 
 
 
Обычные харды в k8s не поддерживаются. Работа с дисками происходит по средствам распределенных файловых хранилищ. Это необходимо для того, чтобы k8s мог «перемещать» контейнеры docker на другие ноды в случае необходимости, без потери данных (файлов). 
 
Начинать создание кластера нужно именно с создания своего распределенного файлового хранилища. Если вы уверены, что диски вам никогда не понадобятся, то этот шаг можно пропустить. 
Я выбрал  . А еще рекомендую почитать эту  . 
 
Минимальное разумное количество серверов для Ceph — 3 (можно построить и на одном, но в этом мало смысла из-за высокой вероятности потерять данные). 
 
 
Нам понадобится Flannel — он позволяет организовать программно определяемую сеть (Software Defined Network, SDN). Именно SDN позволяет всем нашим контейнерам общаться с друг другом внутри кластера (установка Flannel производится вместе с k8s и описана ниже). 
 
 
В нашем примере мы используем 3 физических сервера. Установите Ubuntu 16.04 на все сервера. Не создавайте   партиции (требование k8s).  
 
Предусмотрите в каждом сервере как минимум один диск (или партицию) для Ceph. 
 
Не включайте поддержку SELinux (в Ubuntu 16.04 он выключен по-умолчанию). 
 
Мы назвали сервера так: kub01 kub02 kub03. Партиция sda2 на каждом сервере создана для Ceph (форматировать не обязательно). 
 
 
Установку Ceph я опишу довольно кратко. В сети много примеров и на самом сайте   довольно хорошая документация. 
 
Все операции производим из под привелигированого пользователя root. 
 
Создадим временную директорию:  
 
 
Установим Ceph: 
 
 
Необходимо создать ключ и разложить его по всем серверам. Это нужно для утилиты ceph-deploy: 
 
 
 
(возможно, вам понадобится поправить конфиг ssh для разрешения логинится под пользователем root). 
 
Проверьте, что kub01 не прописан в вашем /etc/hosts как 127.0.0.1 (если прописан, удалите эту строку). 
 
Создаем дисковый кластер и инициализируем его: 
 
 
Проверяем наш дисковый кластер: 
 
 
Можно взять на вооружение следующие полезные команды: 
 
 
Теперь, когда мы убедились, что Ceph работает, мы создадим отдельный пул (pool) для k8s: 
 
 
(можно посмотреть все существующие пулы командой: ceph df) 
 
Теперь создадим отдельного пользователя для нашего пула   и сохраним ключи: 
 
 
(ключи понадобятся для доступа k8s к хранилищу) 
 
 
Добавим репозиторий k8s в нашу систему: 
 
 
Теперь установим основные пакеты: 
 
 
Инициализируем и запускаем k8s 
 
 
(именно такая сеть 10.244.0.0/16 необходима для работы flannel — не изменяйте ее) 
 
Сохраните напечатанную скриптом команду для присоединения нод к кластеру. 
 
Для работы с k8s удобно использовать отдельного непривилегированного пользователя. Создадим его и скопируем в него конфигурационный файл k8s: 
 
 
Для работы с k8s используется утилита:  . Используем ее только из под нашего пользователя  . Для перехода под пользователя выполним: 
 
 
Разрешаем запуск контейнеров на мастере: 
 
 
Настраиваем права: 
 
 
Устанавливаем flannel (сетевую подсистему): 
 
 
 
 
 
 
 
Запустим kube-proxy, сделать это можно так: 
 
 
И пробросим порт 8001 с вашей рабочей машины до сервера kub01: 
 
 
Теперь мы можем зайти в веб интерфейс со своей рабочей машины по адресу: 
 
 (откроется веб интерфейс, где нужно указать токен) 
 
Получить токен для доступа в веб интерфейс можно так: 
 
 
 
В текущем контроллере   отсутствует бинарник  , необходимый для работы с  , поэтому создадим свой собственный kube-controller-manager: 
 
 
Для создания нашего   сделайте следующее: 
(все команды выполняем из под пользователя root) 
 
 
(обязательно указывайте актуальные версии k8s и дистрибутива ОС) 
 
Проверяем, что наш контроллер успешно создан: 
 
 
Проверяем, что в нашем образе есть rbd: 
 
 
Должны увидеть что-то вида: rbd: /usr/bin/rbd /usr/share/man/man8/rbd.8.gz 
 
Заменяем стандартный контроллер нашим, для этого правим файл: 
 
 
Заменяем строку: 
 на:  
 
(обязательно добавляем директиву  , чтобы k8s не пытался скачивать этот образ из интернета) 
 
 
 
 
 
Image: my-kube-controller-manager:v1.9.2 
 
Теперь, когда запустился наш контроллер с поддержкой RBD, мы можем приступить к настройке связки k8s и Ceph. 
 
 
 
 
 
 
 
 
 
 
На новом сервере выполните следующие команды: 
 
 
 
Нам нужен ключ. Его можно получить на мастере, выполнив команду: 
 
 
Либо, создать его: 
 
 
 
 
 
 
Для быстрой и простой установки приложений в k8s был придуман Helm. 
 
Список доступных приложений можно найти  . 
 
 
 
 
P.S.: на написание этой статьи ушло 6 часов. Не судите строго, если где-то есть опечатки. Задавайте вопросы, с радостью отвечу и помогу.
